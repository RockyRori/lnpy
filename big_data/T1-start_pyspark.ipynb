{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WULml9444eMs",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Tutorial 1: Familar yourself with Pyspark\n",
        "A good programming platform can save you lots of troubles and time. \n",
        "Try to setup the environment to start Pyspark.\n",
        "This tutorial will present the easiest way to run Pyspark on a Juypter Notebook. \n",
        "The whole course will be based on Juypter Notebook.\n",
        "If you want to install on the other operator system, you can see the manual attached with this course. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj_dOmOoPM8F"
      },
      "source": [
        "## 1.0 Install Required Library \n",
        "Everytime you use the Python engine provided by Google Cloud, you need to install all the libraries to run the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QQ5c3f2PPvBA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in c:\\configuration\\python\\python312\\lib\\site-packages (3.5.4)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in c:\\configuration\\python\\python312\\lib\\site-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEEgJF2x4eMu",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 1.1 Create SparkContext and SparkSession\n",
        "Before we look into examples, first let’s initialize **SparkSession** and **SparkContext** using the builder pattern method defined in SparkSession class. \n",
        "While initializing, we need to provide the master and application name as shown below. \n",
        "In realtime application, you will pass master from spark-submit instead of hardcoding on Spark application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1639378790796
        },
        "id": "gexT6VlX4eMv",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# create entry points to spark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "ss  = SparkSession.builder \\\n",
        "                            .master(\"local[1]\")\\\n",
        "                            .appName(\"SparkByExamples.com\")\\\n",
        "                            .getOrCreate()\n",
        "spark = ss.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZiFxok84eMw",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "The parameters in the Session stands for:\n",
        "\n",
        "`master()` – If you are running it on the cluster you need to use your master name as an argument to master(). usually, it would be either <a href=\"https://sparkbyexamples.com/hadoop/how-yarn-works/\">yarn (Yet Another Resource Negotiator)</a> or mesos depends on your cluster setup.\n",
        "- Use `local[x]` when running in Standalone mode (i.e., local machine). x should be an integer value and should be greater than 0; this represents how many partitions it should create when using RDD, DataFrame, and Dataset. Ideally, x value should be the number of CPU cores you have.\n",
        "\n",
        "`appName()` – Used to set your application name.\n",
        "\n",
        "`getOrCreate()` – This returns a SparkSession object if already exists, creates new one if not exists."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgFqAVPY4eMx",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 1.2 SparkContext\n",
        "SparkContext is a main entry point for Spark functionality. \n",
        "A SparkContext represents the connection to a Spark cluster, and can be used to create RDDs, accumlators and broadcast variables on that cluster.\n",
        "\n",
        "Here, we will try to get the setting in the current SparkContext."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1639382807575
        },
        "id": "35sfkJth4eMx",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "spark.getConf().getAll()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHNiuDim4eMy",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 1.3 Log Levels\n",
        "When running operations on Pyspark, there will be many **log** giving the information of different ops. \n",
        "We can adjust the **log level** to control the information displayed. \n",
        "By default, the log level is **warn** in Pyspark. The log level in Pyspark is:\n",
        "\n",
        "|  Log Levels   | Meanings  |\n",
        "|  ----  | ----  |\n",
        "| DEBUG  | The DEBUG Level designates fine-grained infomration events that are most useful to debug an application |\n",
        "| INFO  | The INFO level designates infomrational messages that highlights the progresss of the application at coarse-grained level. |\n",
        "| WARN  | The WARN level designates potentially harmful situations. |\n",
        "| ERROR  | The ERROR level designates error events that might still allow the application to continue running. |\n",
        "| TRACE  | The TRACE Level designates finer-grained informational events than the DEBUG. |\n",
        "| FATAL  | The FATAL Level designates very severe error events that will presumably lead the application to abort. |\n",
        "| ALL  | The ALL Level has the lowest possible rank and is intended to turn on all logging. |\n",
        "| OFF  | The OFF Level has the highest possible rank and is intended to turn off logging. |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1639383578317
        },
        "id": "Ms62UWKf4eMy",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# to print all information in operation, you can set log level to ALL\n",
        "spark.setLogLevel(\"ALL\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gs3-OCbb4eMz",
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 1.4 Hello world\n",
        "If you can run the following code, that means the Pyspark is running. Let's run a simple program to do a letter count. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1639378802654
        },
        "id": "8EaoZ0LN4eMz",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext\n",
        "from operator import add\n",
        " \n",
        "\n",
        "data = spark.parallelize(list(\"Hello World\"))\n",
        "counts = data.map(lambda x: \n",
        "\t(x, 1)).reduceByKey(add).sortBy(lambda x: x[1],\n",
        "\t ascending=False).collect()\n",
        "\n",
        "for (word, count) in counts:\n",
        "    print(\"{}: {}\".format(word, count))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "T1-start_pyspark.ipynb",
      "provenance": []
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
